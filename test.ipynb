{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    KFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Model imports\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    Lasso\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = pd.read_csv('rmpCapstoneNum.csv', header=None)\n",
    "qual = pd.read_csv('rmpCapstoneQual.csv', header=None)\n",
    "tag = pd.read_csv('rmpCapstoneTags.csv', header=None)\n",
    "\n",
    "num_columns = [\n",
    "    \"Average Rating\",            # The arithmetic mean of all individual quality ratings of this professor\n",
    "    \"Average Difficulty\",        # The arithmetic mean of all individual difficulty ratings of this professor\n",
    "    \"Number of Ratings\",         # Total number of ratings these averages are based on\n",
    "    \"Received a 'pepper'?\",      # Boolean - judged as \"hot\" by the students\n",
    "    \"Proportion Retake\",         # Proportion of students that said they would take the class again\n",
    "    \"Online Ratings Count\",      # Number of ratings coming from online classes\n",
    "    \"Male Gender\",               # Boolean – 1: determined with high confidence that professor is male\n",
    "    \"Female Gender\"              # Boolean – 1: determined with high confidence that professor is female\n",
    "]\n",
    "num.columns = num_columns\n",
    "\n",
    "qual_columns = [\n",
    "    \"Major/Field\",  # Column 1: Major/Field\n",
    "    \"University\",   # Column 2: University\n",
    "    \"US State\"      # Column 3: US State (2-letter abbreviation)\n",
    "]\n",
    "qual.columns = qual_columns\n",
    "\n",
    "tags_columns = [\n",
    "    \"Tough grader\",              # Column 1\n",
    "    \"Good feedback\",             # Column 2\n",
    "    \"Respected\",                 # Column 3\n",
    "    \"Lots to read\",              # Column 4\n",
    "    \"Participation matters\",     # Column 5\n",
    "    \"Don't skip class\",          # Column 6\n",
    "    \"Lots of homework\",          # Column 7\n",
    "    \"Inspirational\",             # Column 8\n",
    "    \"Pop quizzes!\",              # Column 9\n",
    "    \"Accessible\",                # Column 10\n",
    "    \"So many papers\",            # Column 11\n",
    "    \"Clear grading\",             # Column 12\n",
    "    \"Hilarious\",                 # Column 13\n",
    "    \"Test heavy\",                # Column 14\n",
    "    \"Graded by few things\",      # Column 15\n",
    "    \"Amazing lectures\",          # Column 16\n",
    "    \"Caring\",                    # Column 17\n",
    "    \"Extra credit\",              # Column 18\n",
    "    \"Group projects\",            # Column 19\n",
    "    \"Lecture heavy\"              # Column 20\n",
    "]\n",
    "tag.columns = tags_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the datasets have the same number of records\n",
    "assert len(num) == len(qual) == len(tag), \"Datasets lengths do not match.\"\n",
    "\n",
    "# Merge the datasets\n",
    "merged_df = pd.concat([num, qual, tag], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89893\n",
      "25368\n"
     ]
    }
   ],
   "source": [
    "# set the threshold to 5 ratings and exclude the professors with less than 5 ratings\n",
    "print(len(merged_df))\n",
    "merged_df = merged_df[merged_df['Number of Ratings'] >= 5]\n",
    "print(len(merged_df))\n",
    "\n",
    "merged_df = merged_df.dropna(subset='Average Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_rating = merged_df[merged_df['Male Gender'] == 0]['Average Rating']\n",
    "male_rating = merged_df[merged_df['Male Gender'] == 1]['Average Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 10.7285\n",
      "p-value: 0.0000\n",
      "\n",
      "Male rating mean: 3.9141\n",
      "Female rating mean: 3.7866\n"
     ]
    }
   ],
   "source": [
    "# run welch's t-test\n",
    "t_stat, p_value = stats.ttest_ind(male_rating, female_rating, \n",
    "                                 alternative='greater',  # one-sided test\n",
    "                                 equal_var=False)  # Welch's t-test (unequal variances)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Print means for reference\n",
    "print(f\"\\nMale rating mean: {male_rating.mean():.4f}\")\n",
    "print(f\"Female rating mean: {female_rating.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "# For each tag column\n",
    "for tag_column in tags_columns:\n",
    "    # Create new normalized column\n",
    "    merged_df[f'{tag_column}_normalized'] = merged_df[tag_column] / merged_df['Number of Ratings']\n",
    "# create a list of normalized cols\n",
    "normalized_columns = [f'{tag_column}_normalized' for tag_column in tags_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for tags in normalized_columns:\n",
    "    male = merged_df[merged_df['Male Gender'] == 1][tags]\n",
    "    female = merged_df[merged_df['Male Gender'] == 0][tags]\n",
    "    t_stat, p_value = stats.mannwhitneyu(male, female, \n",
    "                                 alternative='two-sided',  # one-sided test\n",
    "                                 )  # Welch's t-test (unequal variances)\n",
    "    result.append((tags, p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hilarious_normalized', 8.499323999196362e-206),\n",
       " ('Amazing lectures_normalized', 1.851816834600291e-44),\n",
       " ('Respected_normalized', 3.3533262258853186e-32),\n",
       " ('Lots of homework_normalized', 1.509977533642015e-25),\n",
       " ('Caring_normalized', 6.572070748455326e-23),\n",
       " ('Extra credit_normalized', 4.242282844390809e-22),\n",
       " ('Participation matters_normalized', 5.184174055707606e-21),\n",
       " ('Group projects_normalized', 2.242054641710866e-19),\n",
       " ('Graded by few things_normalized', 6.14348410119504e-14),\n",
       " (\"Don't skip class_normalized\", 1.9626175039896763e-10),\n",
       " ('Tough grader_normalized', 3.1725123597144213e-09),\n",
       " ('Lecture heavy_normalized', 1.830223017094474e-08),\n",
       " ('Inspirational_normalized', 1.3951632901818763e-05),\n",
       " ('Lots to read_normalized', 0.0019799102664822474),\n",
       " ('So many papers_normalized', 0.010550743119841204),\n",
       " ('Accessible_normalized', 0.034012681477221945),\n",
       " ('Good feedback_normalized', 0.034968507886676686),\n",
       " ('Clear grading_normalized', 0.03896696872117285),\n",
       " ('Pop quizzes!_normalized', 0.13358788872561184),\n",
       " ('Test heavy_normalized', 0.23905318708096235)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the results by p-value\n",
    "result.sort(key=lambda x: x[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
